{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data loading and spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 80\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "percent_data_select = \"train[:100]\" # add percent sign ie. \"train[:20%]\" to select that percent of data \n",
    "# Load only 20% of the dataset\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"../Datasets/processed_data.csv\"}, split=percent_data_select)\n",
    "\n",
    "# Split into train and test sets (e.g., 80% train, 20% test)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Further split the test set into validation and test (e.g., 50-50 split of the 20%)\n",
    "validation_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Combine splits into a DatasetDict\n",
    "raw_dataset = {\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": validation_test_split[\"train\"],\n",
    "    \"test\": validation_test_split[\"test\"]\n",
    "}\n",
    "\n",
    "dataset = DatasetDict(raw_dataset)\n",
    "\n",
    "# Inspect the resulting dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'English': 'for more detailed figures from 2011 census, see this table.', 'Hindi': '2011 की जनगणना से अधिक विस्तृत आंकड़ों के लिए, इस तालिका को देखें।'}\n",
      "{'English': 'death is seen as a boundary to another world.', 'Hindi': 'मौत एक और दुनिया के लिए एक सीमा के रूप में देखा जाता है।'}\n",
      "{'English': 'george miller always wanted one person to do both .', 'Hindi': 'जॉर्ज मिलर हमेशा एक व्यक्ति को दोनों करना चाहते थे।'}\n",
      "{'English': 'the targets of the german aircraft were actually the rail lines and bridges.', 'Hindi': 'जर्मन विमानों के लक्ष्य वास्तव में रेल लाइन और पुल थे।'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])\n",
    "print(dataset['train'][1])\n",
    "print(dataset['train'][2])\n",
    "print(dataset['train'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 2.\tFacebook / M2M100 418M\n",
    "- M2M-100 stands for Massively Multilingual Model with 100 languages.\n",
    "- 418M refers to the model size in terms of the number of parameters. Parameters are the learnable weights within the model's neural network. A larger number of parameters generally allows the model to learn more complex patterns and achieve higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Comparing Tokenizers \n",
    "- Our approach will involve a systematic comparison of tokenization strategies, paying close attention to how each method handles linguistic characteristics specific to Hindi and English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig, AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_ID = \"facebook/m2m100_418M\"  # Replace with your desired model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ID)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ID, src_lang=\"en\", tgt_lang=\"hi\")\n",
    "# Create a GenerationConfig with the desired parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Hindi Input: translate Hindi to English: इस काम के बोझ को मेरे साथ कौन साझा करेगा?\n",
      "Input Tokens: ['__en__', '▁trans', 'late', '▁Hindi', '▁to', '▁English', ':', '▁इस', '▁काम', '▁के', '▁बो', 'झ', '▁को', '▁मेरे', '▁साथ', '▁कौन', '▁सा', 'झा', '▁करे', 'गा', '?', '</s>']\n",
      "Decoded Input Text: __en__ translate Hindi to English: इस काम के बोझ को मेरे साथ कौन साझा करेगा?</s>\n",
      "\n",
      "Original English Target: who will share the burden of this work with me?\n",
      "Target Tokens: ['__hi__', '▁who', '▁will', '▁share', '▁the', '▁bur', 'den', '▁of', '▁this', '▁work', '▁with', '▁me', '?', '</s>']\n",
      "Decoded Target Text: __hi__ who will share the burden of this work with me?</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akthe\\miniconda3\\envs\\WPanda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3957: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Sample data from the dataset\n",
    "sample = dataset[\"train\"][12]  # Replace with an actual sample index\n",
    "input_text = \"translate Hindi to English: \" + sample[\"Hindi\"]\n",
    "target_text = sample[\"English\"]\n",
    "\n",
    "# Tokenize inputs and targets\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(target_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Convert token IDs back to tokens\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "target_tokens = tokenizer.convert_ids_to_tokens(targets[\"input_ids\"][0])\n",
    "\n",
    "# Print the results for inspection\n",
    "print(\"Original Hindi Input:\", input_text)\n",
    "print(\"Input Tokens:\", input_tokens)\n",
    "print(\"Decoded Input Text:\", tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=False))\n",
    "print(\"\\nOriginal English Target:\", target_text)\n",
    "print(\"Target Tokens:\", target_tokens)\n",
    "print(\"Decoded Target Text:\", tokenizer.decode(targets[\"input_ids\"][0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Why are \"__hi__\" and \"__en__\" appearing before the English and Hindi sentences, respectively?**\n",
    "\n",
    "These special tokens, \"__hi__\" and \"__en__\", are often used in machine translation tasks to explicitly indicate the **source language** and **target language**. They serve as markers for the model to understand which language it's currently processing.\n",
    "\n",
    "##### Here's a breakdown of their functions:\n",
    "\n",
    "* **__hi__ and __en__: Language Identifiers:**\n",
    "   - **__hi__**: This token typically represents the Hindi language. When placed at the beginning of a sentence, it tells the model that the following text is in Hindi and should be translated into the target language (in this case, English).\n",
    "   - **__en__**: Similarly, \"__en__\" indicates that the following text is in English. It might be used in scenarios where the model is asked to translate from English to Hindi or for tasks like language identification.\n",
    "\n",
    "##### Why are they added?\n",
    "\n",
    "* **Clarity for the Model:** These markers provide clear and explicit information to the model about the language of the input and output sequences. This helps the model to better understand the context and improve the accuracy of its translations.\n",
    "* **Handling Multiple Language Pairs:** In multilingual models, these tokens can be used to handle various language pairs. For example, if the model is trained on multiple language pairs (e.g., English-French, English-Spanish), these tokens can help the model distinguish between the different language pairs.\n",
    "* **Facilitating Language Identification:** In some cases, these tokens can also be used for language identification tasks, where the model is asked to determine the language of a given text.\n",
    "\n",
    "##### In above given example:\n",
    "\n",
    "* **\"__en__ translate Hindi to English: जॉर्ज मिलर हमेशा एक व्यक्ति को दोनों करना चाहते थे।\"**: This part tells the model that the following text is in English and the task is to translate the Hindi text that follows.\n",
    "* **\"__hi__ george miller always wanted one person to do both .\"**: This indicates that the following text is the English translation of the preceding Hindi text.\n",
    "\n",
    "**Note:** The specific tokens used (e.g., \"__hi__\", \"__en__\") can vary depending on the pre-training data and the specific configuration of the model. However, the general concept of using special tokens to indicate language remains consistent.\n",
    "\n",
    "By understanding the role of these tokens, one can better interpret the model's output and fine-tune your training data for more accurate translations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 M2M100 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import M2M100Config, M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "model_ID_fb = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "tokenizer_fb = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"en\", tgt_lang=\"hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Hindi Input: translate Hindi to English: इस काम के बोझ को मेरे साथ कौन साझा करेगा?\n",
      "Input Tokens: ['__en__', '▁trans', 'late', '▁Hindi', '▁to', '▁English', ':', '▁इस', '▁काम', '▁के', '▁बो', 'झ', '▁को', '▁मेरे', '▁साथ', '▁कौन', '▁सा', 'झा', '▁करे', 'गा', '?', '</s>']\n",
      "Decoded Input Text: __en__ translate Hindi to English: इस काम के बोझ को मेरे साथ कौन साझा करेगा?</s>\n",
      "\n",
      "Original English Target: who will share the burden of this work with me?\n",
      "Target Tokens: ['__hi__', '▁who', '▁will', '▁share', '▁the', '▁bur', 'den', '▁of', '▁this', '▁work', '▁with', '▁me', '?', '</s>']\n",
      "Decoded Target Text: __hi__ who will share the burden of this work with me?</s>\n"
     ]
    }
   ],
   "source": [
    "# Sample data from the dataset\n",
    "sample2 = dataset[\"train\"][12]  # Replace with an actual sample index\n",
    "input_text2 = \"translate Hindi to English: \" + sample[\"Hindi\"]\n",
    "target_text2 = sample[\"English\"]\n",
    "\n",
    "# Tokenize inputs and targets\n",
    "inputs2 = tokenizer_fb(input_text2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with tokenizer_fb.as_target_tokenizer():\n",
    "    targets2 = tokenizer_fb(target_text2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Convert token IDs back to tokens\n",
    "input_tokens2 = tokenizer_fb.convert_ids_to_tokens(inputs2[\"input_ids\"][0])\n",
    "target_tokens2 = tokenizer_fb.convert_ids_to_tokens(targets2[\"input_ids\"][0])\n",
    "\n",
    "# Print the results for inspection\n",
    "print(\"Original Hindi Input:\", input_text2)\n",
    "print(\"Input Tokens:\", input_tokens2)\n",
    "print(\"Decoded Input Text:\", tokenizer_fb.decode(inputs2[\"input_ids\"][0], skip_special_tokens=False))\n",
    "print(\"\\nOriginal English Target:\", target_text2)\n",
    "print(\"Target Tokens:\", target_tokens2)\n",
    "print(\"Decoded Target Text:\", tokenizer_fb.decode(targets2[\"input_ids\"][0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'additional_special_tokens': ['__af__', '__am__', '__ar__', '__ast__', '__az__', '__ba__', '__be__', '__bg__', '__bn__', '__br__', '__bs__', '__ca__', '__ceb__', '__cs__', '__cy__', '__da__', '__de__', '__el__', '__en__', '__es__', '__et__', '__fa__', '__ff__', '__fi__', '__fr__', '__fy__', '__ga__', '__gd__', '__gl__', '__gu__', '__ha__', '__he__', '__hi__', '__hr__', '__ht__', '__hu__', '__hy__', '__id__', '__ig__', '__ilo__', '__is__', '__it__', '__ja__', '__jv__', '__ka__', '__kk__', '__km__', '__kn__', '__ko__', '__lb__', '__lg__', '__ln__', '__lo__', '__lt__', '__lv__', '__mg__', '__mk__', '__ml__', '__mn__', '__mr__', '__ms__', '__my__', '__ne__', '__nl__', '__no__', '__ns__', '__oc__', '__or__', '__pa__', '__pl__', '__ps__', '__pt__', '__ro__', '__ru__', '__sd__', '__si__', '__sk__', '__sl__', '__so__', '__sq__', '__sr__', '__ss__', '__su__', '__sv__', '__sw__', '__ta__', '__th__', '__tl__', '__tn__', '__tr__', '__uk__', '__ur__', '__uz__', '__vi__', '__wo__', '__xh__', '__yi__', '__yo__', '__zh__', '__zu__']}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.special_tokens_map)  # Check special tokens (e.g., <pad>, <unk>, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'additional_special_tokens': ['__af__', '__am__', '__ar__', '__ast__', '__az__', '__ba__', '__be__', '__bg__', '__bn__', '__br__', '__bs__', '__ca__', '__ceb__', '__cs__', '__cy__', '__da__', '__de__', '__el__', '__en__', '__es__', '__et__', '__fa__', '__ff__', '__fi__', '__fr__', '__fy__', '__ga__', '__gd__', '__gl__', '__gu__', '__ha__', '__he__', '__hi__', '__hr__', '__ht__', '__hu__', '__hy__', '__id__', '__ig__', '__ilo__', '__is__', '__it__', '__ja__', '__jv__', '__ka__', '__kk__', '__km__', '__kn__', '__ko__', '__lb__', '__lg__', '__ln__', '__lo__', '__lt__', '__lv__', '__mg__', '__mk__', '__ml__', '__mn__', '__mr__', '__ms__', '__my__', '__ne__', '__nl__', '__no__', '__ns__', '__oc__', '__or__', '__pa__', '__pl__', '__ps__', '__pt__', '__ro__', '__ru__', '__sd__', '__si__', '__sk__', '__sl__', '__so__', '__sq__', '__sr__', '__ss__', '__su__', '__sv__', '__sw__', '__ta__', '__th__', '__tl__', '__tn__', '__tr__', '__uk__', '__ur__', '__uz__', '__vi__', '__wo__', '__xh__', '__yi__', '__yo__', '__zh__', '__zu__']}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_fb.special_tokens_map)  # Check special tokens (e.g., <pad>, <unk>, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Both tokenizers i.e. pretrained models and model-specific tokenizers can yield similar outputs in many scenarios, but this isn't a universal rule.**\n",
    "\n",
    "  -  ***When comparing different models, such as `M2M100` and `Google's T5 small`, we'll encounter variations in how unknown words are processed. The Google T5 small model, for instance, tends to generate more `<unk>` tokens when encountering vocabulary outside its training set, which can pose challenges for translation*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 text preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Importance of Text Preprocessing:**\n",
    "\n",
    "1. **Converting Text to Numbers:** Machine learning models can't directly understand raw text. Preprocessing transforms text into numerical representations (tokens) that the model can process.\n",
    "\n",
    "2. **Normalization and Consistency:** Text data can have inconsistencies like capitalization, punctuation, and variations in word forms (e.g., singular vs. plural). Preprocessing steps like lowercasing or stemming/lemmatization can address these issues, promoting consistency in the data.\n",
    "\n",
    "3. **Feature Engineering:** Preprocessing can create new features for the model. In your example, prepending \"translate Hindi to English: \" to the source sentences might help the model understand the context of translation.\n",
    "\n",
    "4. **Handling Text Length:** Different models have limitations on input and output lengths. Preprocessing techniques like truncation and padding ensure your data adheres to these limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [\"translate Hindi to English: \" + ex for ex in examples[\"Hindi\"]]\n",
    "    targets = examples[\"English\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['English', 'Hindi', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['English', 'Hindi', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['English', 'Hindi', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Hindi: 2011 की जनगणना से अधिक विस्तृत आंकड़ों के लिए, इस तालिका को देखें।\n",
      "Original English: for more detailed figures from 2011 census, see this table.\n",
      "Tokenized Input IDs: [128022, 5815, 80447, 11631, 128, 18006, 9, 2294, 783, 15258, 2568, 3207, 1383, 1044, 21283, 118809, 58278, 1843, 15694, 8967, 1839, 456, 3460, 4, 5163, 48867, 8964, 929, 78705, 209, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Input: __en__ translate Hindi to English: 2011 की जनगणना से अधिक विस्तृत आंकड़ों के लिए, इस तालिका को देखें।</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Tokenized Label IDs: [128036, 193, 8933, 50939, 241, 119922, 15592, 2294, 57940, 81, 4, 6267, 15911, 49653, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Label: __hi__ for more detailed figures from 2011 census, see this table.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "==========\n",
      "Original Hindi: मौत एक और दुनिया के लिए एक सीमा के रूप में देखा जाता है।\n",
      "Original English: death is seen as a boundary to another world.\n",
      "Tokenized Input IDs: [128022, 5815, 80447, 11631, 128, 18006, 9, 51178, 1618, 1544, 44635, 456, 3460, 1618, 76231, 456, 25753, 698, 42021, 27741, 776, 209, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Input: __en__ translate Hindi to English: मौत एक और दुनिया के लिए एक सीमा के रूप में देखा जाता है।</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Tokenized Label IDs: [128036, 120849, 117, 74322, 285, 8, 502, 1535, 3381, 128, 122000, 55185, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Label: __hi__ death is seen as a boundary to another world.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "==========\n",
      "Original Hindi: जॉर्ज मिलर हमेशा एक व्यक्ति को दोनों करना चाहते थे।\n",
      "Original English: george miller always wanted one person to do both .\n",
      "Tokenized Input IDs: [128022, 5815, 80447, 11631, 128, 18006, 9, 2513, 8173, 3830, 1619, 23862, 1686, 10703, 98394, 1618, 26054, 929, 56551, 26211, 80992, 19769, 209, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Input: __en__ translate Hindi to English: जॉर्ज मिलर हमेशा एक व्यक्ति को दोनों करना चाहते थे।</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Tokenized Label IDs: [128036, 1018, 42284, 6457, 74, 119511, 124350, 13331, 3419, 128, 61, 118813, 237, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded Label: __hi__ george miller always wanted one person to do both .</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Inspect the first example\n",
    "for idx in range(3):  # Print first 3 examples as a sample\n",
    "    print(f\"Original Hindi: {dataset['train'][idx]['Hindi']}\")\n",
    "    print(f\"Original English: {dataset['train'][idx]['English']}\")\n",
    "\n",
    "    # Tokenized inputs\n",
    "    tokenized_input = tokenized_datasets[\"train\"][idx][\"input_ids\"]\n",
    "    print(f\"Tokenized Input IDs: {tokenized_input}\")\n",
    "    print(f\"Decoded Input: {tokenizer.decode(tokenized_input, skip_special_tokens=False)}\")\n",
    "\n",
    "    # Tokenized outputs\n",
    "    tokenized_label = tokenized_datasets[\"train\"][idx][\"labels\"]\n",
    "    print(f\"Tokenized Label IDs: {tokenized_label}\")\n",
    "    print(f\"Decoded Label: {tokenizer.decode(tokenized_label, skip_special_tokens=False)}\")\n",
    "\n",
    "    print(\"=\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specific Breakdown to Code:**\n",
    "\n",
    "- **`inputs = [\"translate Hindi to English: \" + ex for ex in examples[\"Hindi\"]]`**: This line creates a new list (`inputs`) by prepending a context string to each sentence in the `Hindi` column of the dataset.\n",
    "\n",
    "- **`model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")`**: This tokenizes the `inputs` list, converting them into numerical representations using the `tokenizer`. The `max_length` argument limits the length of each sequence, and `truncation=True` ensures sequences exceeding the limit are shortened. `padding=\"max_length\"` pads shorter sequences with special tokens to create a uniform length.\n",
    "\n",
    "- **`with tokenizer.as_target_tokenizer():`**: This context manager configures the tokenizer for handling the target language (English) by setting the appropriate attributes.\n",
    "\n",
    "- **`labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")`**: This tokenizes the target sentences (`English`) with similar parameters as the input.\n",
    "\n",
    "- **`model_inputs[\"labels\"] = labels[\"input_ids\"]`**: This adds the tokenized target sentence IDs (stored as `input_ids` in the `labels` dictionary) as a new key \"labels\" within the `model_inputs` dictionary.\n",
    "\n",
    "- **`tokenized_datasets = dataset.map(preprocess_function, batched=True)`**: This line applies the `preprocess_function` to each element of the dataset (`dataset`) in batches using `batched=True` for efficiency. The resulting processed data is stored in `tokenized_datasets`.\n",
    "\n",
    "In summary, this text preprocessing step transforms your raw text data into a format suitable for training your machine translation model. It ensures consistency, handles sequence lengths, and potentially adds contextual information to aid the translation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data collator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, particularly for transformer models, a data collator plays a crucial role in preparing batches of data for training. It's essentially a function that takes individual samples and combines them into batches in a way that's efficient and optimized for the model's processing.\n",
    "\n",
    "\n",
    "For sequence-to-sequence tasks like translation, a specialized data collator (often DataCollatorForSeq2Seq) becomes critical. It ensures that:\n",
    "- Source and target sequences are properly aligned\n",
    "- Padding is applied consistently\n",
    "- Attention mechanisms can correctly ignore padded tokens\n",
    "- Labels are prepared in a format that allows for loss calculation during training\n",
    "\n",
    "\n",
    "\n",
    "Without a proper data collator, you'd need to manually handle sequence padding, masking, and batch preparation, which would be computationally expensive and error-prone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 BLEU (Bilingual Evaluation Understudy): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Fine Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akthe\\AppData\\Local\\Temp\\ipykernel_2948\\4159182302.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"../Model/Base/Checkpoint/\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, #change to bf16=True for XPU\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Model/Base/M2M100/tokenizer_config.json',\n",
       " '../Model/Base/M2M100/special_tokens_map.json',\n",
       " '..\\\\Model\\\\Base\\\\M2M100\\\\vocab.json',\n",
       " '..\\\\Model\\\\Base\\\\M2M100\\\\sentencepiece.bpe.model',\n",
       " '../Model/Base/M2M100/added_tokens.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"../Model/Base/M2M100/\")\n",
    "tokenizer.save_pretrained(\"../Model/Base/M2M100/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_FT = AutoModelForSeq2SeqLM.from_pretrained(\"../Model/Base/M2M100/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../Model/Base/M2M100/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'नमस्ते, आप कैसे हैं?'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the translation pipeline\n",
    "translator = pipeline(\"translation_en_to_hi\", model=model_FT, tokenizer=tokenizer)\n",
    "\n",
    "# Test translation\n",
    "text = \"Hello, how are you?\"\n",
    "translated_text = translator(text)\n",
    "\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\tFacebook / M2M100 418M + LoRa\n",
    "- LoRA (Low-Rank Adaptation) refers to a parameter-efficient fine-tuning technique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Large Language Models (LLMs), LoRA (Low-Rank Adaptation) refers to a parameter-efficient fine-tuning technique. \n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "* **Fine-tuning:** LLMs are often pre-trained on massive datasets. Fine-tuning involves adapting these pre-trained models to specific tasks or domains using smaller, more relevant datasets.\n",
    "* **Parameter-Efficiency:** Fine-tuning LLMs can be computationally expensive, especially for very large models. LoRA addresses this by significantly reducing the number of parameters that need to be updated during fine-tuning.\n",
    "\n",
    "**How LoRA Works:**\n",
    "\n",
    "Instead of fine-tuning all the parameters of the base LLM, LoRA introduces two small, trainable matrices (A and B) for each attention layer:\n",
    "\n",
    "1. **Decomposition:** The update to the original weight matrix (W) is approximated as the product of these two smaller matrices: W' = W + A * B.\n",
    "2. **Reduced Parameters:** Since A and B have significantly fewer parameters than the original weight matrix, the overall number of trainable parameters is drastically reduced.\n",
    "3. **Fine-tuning:** Only the parameters of A and B are trained during fine-tuning, while the original weights of the base LLM remain frozen.\n",
    "\n",
    "**Benefits of LoRA:**\n",
    "\n",
    "* **Reduced Training Time and Cost:** By training only a small subset of parameters, LoRA significantly reduces training time and computational resources.\n",
    "* **Improved Efficiency:** The smaller number of parameters leads to faster inference times.\n",
    "* **Preserving Base Model:** Since the base model's weights are frozen, it retains its general knowledge and capabilities while being adapted to the specific task.\n",
    "* **Easier Deployment:** Smaller models are easier to deploy and run on devices with limited resources.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "LoRA has been successfully applied to a wide range of LLM fine-tuning tasks, including:\n",
    "\n",
    "* **Domain Adaptation:** Adapting LLMs to specific domains like finance, medicine, or law.\n",
    "* **Task-Specific Fine-tuning:** Fine-tuning LLMs for specific tasks such as question answering, text summarization, and code generation.\n",
    "* **Personalization:** Creating personalized LLMs for individual users or groups.\n",
    "\n",
    "**In summary:**\n",
    "\n",
    "LoRA is a powerful technique that enables efficient and effective fine-tuning of LLMs. By significantly reducing the number of trainable parameters, LoRA makes it possible to customize large models for specific applications while minimizing training costs and preserving the valuable knowledge of the base model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " M2M100ForConditionalGeneration(\n",
      "  (model): M2M100Model(\n",
      "    (shared): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "    (encoder): M2M100Encoder(\n",
      "      (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x M2M100EncoderLayer(\n",
      "          (self_attn): M2M100SdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): ReLU()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): M2M100Decoder(\n",
      "      (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x M2M100DecoderLayer(\n",
      "          (self_attn): M2M100SdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): M2M100SdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=128112, bias=False)\n",
      ")\n",
      "model M2M100Model(\n",
      "  (shared): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "  (encoder): M2M100Encoder(\n",
      "    (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "    (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x M2M100EncoderLayer(\n",
      "        (self_attn): M2M100SdpaAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): ReLU()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): M2M100Decoder(\n",
      "    (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "    (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x M2M100DecoderLayer(\n",
      "        (self_attn): M2M100SdpaAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): M2M100SdpaAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "model.shared M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "model.encoder M2M100Encoder(\n",
      "  (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "  (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x M2M100EncoderLayer(\n",
      "      (self_attn): M2M100SdpaAttention(\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (activation_fn): ReLU()\n",
      "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.embed_tokens M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "model.encoder.embed_positions M2M100SinusoidalPositionalEmbedding()\n",
      "model.encoder.layers ModuleList(\n",
      "  (0-11): 12 x M2M100EncoderLayer(\n",
      "    (self_attn): M2M100SdpaAttention(\n",
      "      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (activation_fn): ReLU()\n",
      "    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "model.encoder.layers.0 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.0.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.0.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.0.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.0.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.0.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.0.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.0.activation_fn ReLU()\n",
      "model.encoder.layers.0.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.0.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.0.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.1 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.1.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.1.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.1.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.1.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.1.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.1.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.1.activation_fn ReLU()\n",
      "model.encoder.layers.1.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.1.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.1.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.2 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.2.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.2.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.2.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.2.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.2.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.2.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.2.activation_fn ReLU()\n",
      "model.encoder.layers.2.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.2.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.2.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.3 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.3.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.3.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.3.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.3.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.3.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.3.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.3.activation_fn ReLU()\n",
      "model.encoder.layers.3.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.3.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.3.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.4 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.4.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.4.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.4.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.4.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.4.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.4.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.4.activation_fn ReLU()\n",
      "model.encoder.layers.4.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.4.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.4.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.5 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.5.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.5.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.5.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.5.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.5.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.5.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.5.activation_fn ReLU()\n",
      "model.encoder.layers.5.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.5.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.5.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.6 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.6.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.6.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.6.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.6.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.6.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.6.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.6.activation_fn ReLU()\n",
      "model.encoder.layers.6.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.6.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.6.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.7 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.7.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.7.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.7.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.7.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.7.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.7.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.7.activation_fn ReLU()\n",
      "model.encoder.layers.7.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.7.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.7.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.8 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.8.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.8.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.8.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.8.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.8.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.8.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.8.activation_fn ReLU()\n",
      "model.encoder.layers.8.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.8.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.8.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.9 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.9.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.9.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.9.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.9.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.9.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.9.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.9.activation_fn ReLU()\n",
      "model.encoder.layers.9.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.9.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.9.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.10 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.10.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.10.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.10.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.10.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.10.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.10.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.10.activation_fn ReLU()\n",
      "model.encoder.layers.10.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.10.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.10.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.11 M2M100EncoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): ReLU()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.11.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.11.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.11.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.11.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.11.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.11.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.11.activation_fn ReLU()\n",
      "model.encoder.layers.11.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.11.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.11.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder M2M100Decoder(\n",
      "  (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "  (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x M2M100DecoderLayer(\n",
      "      (self_attn): M2M100SdpaAttention(\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (activation_fn): ReLU()\n",
      "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder_attn): M2M100SdpaAttention(\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.embed_tokens M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
      "model.decoder.embed_positions M2M100SinusoidalPositionalEmbedding()\n",
      "model.decoder.layers ModuleList(\n",
      "  (0-11): 12 x M2M100DecoderLayer(\n",
      "    (self_attn): M2M100SdpaAttention(\n",
      "      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (activation_fn): ReLU()\n",
      "    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder_attn): M2M100SdpaAttention(\n",
      "      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "model.decoder.layers.0 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.0.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.0.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.activation_fn ReLU()\n",
      "model.decoder.layers.0.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.0.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.0.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.0.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.0.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.1 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.1.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.1.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.activation_fn ReLU()\n",
      "model.decoder.layers.1.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.1.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.1.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.1.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.1.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.2 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.2.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.2.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.activation_fn ReLU()\n",
      "model.decoder.layers.2.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.2.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.2.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.2.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.2.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.3 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.3.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.3.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.activation_fn ReLU()\n",
      "model.decoder.layers.3.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.3.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.3.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.3.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.3.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.4 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.4.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.4.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.activation_fn ReLU()\n",
      "model.decoder.layers.4.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.4.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.4.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.4.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.4.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.5 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.5.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.5.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.activation_fn ReLU()\n",
      "model.decoder.layers.5.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.5.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.5.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.5.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.5.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.6 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.6.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.6.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.activation_fn ReLU()\n",
      "model.decoder.layers.6.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.6.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.6.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.6.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.6.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.7 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.7.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.7.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.activation_fn ReLU()\n",
      "model.decoder.layers.7.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.7.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.7.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.7.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.7.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.8 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.8.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.8.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.activation_fn ReLU()\n",
      "model.decoder.layers.8.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.8.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.8.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.8.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.8.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.9 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.9.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.9.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.activation_fn ReLU()\n",
      "model.decoder.layers.9.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.9.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.9.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.9.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.9.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.10 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.10.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.10.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.activation_fn ReLU()\n",
      "model.decoder.layers.10.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.10.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.10.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.10.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.10.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.11 M2M100DecoderLayer(\n",
      "  (self_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): ReLU()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): M2M100SdpaAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.11.self_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.11.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.activation_fn ReLU()\n",
      "model.decoder.layers.11.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.11.encoder_attn M2M100SdpaAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.11.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.11.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.11.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "lm_head Linear(in_features=1024, out_features=128112, bias=False)\n"
     ]
    }
   ],
   "source": [
    "# Access the model's named modules\n",
    "for name, module in model.named_modules():\n",
    "    print(name, module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=4,  # Low rank (fewer trainable parameters)\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Apply LoRA to attention layers (query and value)\n",
    "    bias=\"none\",  # Specify which biases to train\n",
    "    task_type=\"SEQ_2_SEQ_LM\",  # Task type (sequence-to-sequence)\n",
    ")\n",
    "\n",
    "# Wrap the base model with LoRA\n",
    "model_lora = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akthe\\AppData\\Local\\Temp\\ipykernel_2948\\3170028107.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a8fa77f51a4ef98ec6f7b70516502e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f385178ec0544adf9d7721e179d63aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.110392570495605, 'eval_bleu': 1.4304, 'eval_gen_len': 24.6667, 'eval_runtime': 12.3127, 'eval_samples_per_second': 0.487, 'eval_steps_per_second': 0.081, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7dc0804c624b41ba3b81636e8ebb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.103043556213379, 'eval_bleu': 1.4304, 'eval_gen_len': 24.6667, 'eval_runtime': 11.6643, 'eval_samples_per_second': 0.514, 'eval_steps_per_second': 0.086, 'epoch': 2.0}\n",
      "{'train_runtime': 100.8059, 'train_samples_per_second': 0.952, 'train_steps_per_second': 0.06, 'train_loss': 10.42147445678711, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=10.42147445678711, metrics={'train_runtime': 100.8059, 'train_samples_per_second': 0.952, 'train_steps_per_second': 0.06, 'total_flos': 26048741769216.0, 'train_loss': 10.42147445678711, 'epoch': 2.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"../Model/LoRa/Checkpoint/\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, #change to bf16=True for XPU\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lora.merge_and_unload()\n",
    "model_lora.save_pretrained(\"../Model/LoRa/M2M100/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading adapter weights from ../Model/LoRa/M2M100/ led to missing keys in the model: model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight, model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight, model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight, model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight, model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight, model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight, model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight, model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight, model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight, model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight, model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight, model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight, model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight, model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight, model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight, model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight, model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight, model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight, model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight, model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight, model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight, model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight, model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight, model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight, model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight, model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight, model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight, model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight, model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight, model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight, model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight, model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight, model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight, model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight, model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight, model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight, model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight, model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight, model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight, model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight, model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight, model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight, model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight, model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight, model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight, model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight, model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight, model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight, model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight, model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight, model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight, model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight, model.decoder.layers.0.encoder_attn.v_proj.lora_A.default.weight, model.decoder.layers.0.encoder_attn.v_proj.lora_B.default.weight, model.decoder.layers.0.encoder_attn.q_proj.lora_A.default.weight, model.decoder.layers.0.encoder_attn.q_proj.lora_B.default.weight, model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight, model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight, model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight, model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight, model.decoder.layers.1.encoder_attn.v_proj.lora_A.default.weight, model.decoder.layers.1.encoder_attn.v_proj.lora_B.default.weight, model.decoder.layers.1.encoder_attn.q_proj.lora_A.default.weight, model.decoder.layers.1.encoder_attn.q_proj.lora_B.default.weight, model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight, model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight, model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight, model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight, model.decoder.layers.2.encoder_attn.v_proj.lora_A.default.weight, model.decoder.layers.2.encoder_attn.v_proj.lora_B.default.weight, model.decoder.layers.2.encoder_attn.q_proj.lora_A.default.weight, model.decoder.layers.2.encoder_attn.q_proj.lora_B.default.weight, model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight, model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight, model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight, model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight, model.decoder.layers.3.encoder_attn.v_proj.lora_A.default.weight, model.decoder.layers.3.encoder_attn.v_proj.lora_B.default.weight, model.decoder.layers.3.encoder_attn.q_proj.lora_A.default.weight, model.decoder.layers.3.encoder_attn.q_proj.lora_B.default.weight, model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight, model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight, model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight, model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight, model.decoder.layers.4.encoder_attn.v_proj.lora_A.default.weight, model.decoder.layers.4.encoder_attn.v_proj.lora_B.default.weight, model.decoder.layers.4.encoder_attn.q_proj.lora_A.default.weight, model.decoder.layers.4.encoder_attn.q_proj.lora_B.default.weight, model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight, model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight, model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight, model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight, model.decoder.layers.5.encoder_attn.v_proj.lora_A.default.weight, model.decoder.layers.5.encoder_attn.v_proj.lora_B.default.weight, model.decoder.layers.5.encoder_attn.q_proj.lora_A.default.weight, model.decoder.layers.5.encoder_attn.q_proj.lora_B.default.weight, model.decoder.layers.6.self_attn.v_proj.lora_A.default.weight, model.decoder.layers.6.self_attn.v_proj.lora_B.default.weight, model.decoder.layers.6.self_attn.q_proj.lora_A.default.weight, model.decoder.layers.6.self_attn.q_proj.lora_B.default.weight, model.decoder.layers.6.encoder_attn.v_proj.lora_A.default.weight, model.decoder.layers.6.encoder_attn.v_proj.lora_B.default.weight, model.decoder.layers.6.encoder_attn.q_proj.lora_A.default.weight, model.decoder.layers.6.encoder_attn.q_proj.lora_B.default.weight, model.decoder.layers.7.self_attn.v_proj.lora_A.default.weight, model.decoder.layers.7.self_attn.v_proj.lora_B.default.weight, model.decoder.layers.7.self_attn.q_proj.lora_A.default.weight, model.decoder.layers.7.self_attn.q_proj.lora_B.default.weight, model.decoder.layers.7.encoder_attn.v_proj.lora_A.default.weight, model.decoder.layers.7.encoder_attn.v_proj.lora_B.default.weight, model.decoder.layers.7.encoder_attn.q_proj.lora_A.default.weight, model.decoder.layers.7.encoder_attn.q_proj.lora_B.default.weight, model.decoder.layers.8.self_attn.v_proj.lora_A.default.weight, model.decoder.layers.8.self_attn.v_proj.lora_B.default.weight, model.decoder.layers.8.self_attn.q_proj.lora_A.default.weight, model.decoder.layers.8.self_attn.q_proj.lora_B.default.weight, model.decoder.layers.8.encoder_attn.v_proj.lora_A.default.weight, model.decoder.layers.8.encoder_attn.v_proj.lora_B.default.weight, model.decoder.layers.8.encoder_attn.q_proj.lora_A.default.weight, model.decoder.layers.8.encoder_attn.q_proj.lora_B.default.weight, model.decoder.layers.9.self_attn.v_proj.lora_A.default.weight, model.decoder.layers.9.self_attn.v_proj.lora_B.default.weight, model.decoder.layers.9.self_attn.q_proj.lora_A.default.weight, model.decoder.layers.9.self_attn.q_proj.lora_B.default.weight, model.decoder.layers.9.encoder_attn.v_proj.lora_A.default.weight, model.decoder.layers.9.encoder_attn.v_proj.lora_B.default.weight, model.decoder.layers.9.encoder_attn.q_proj.lora_A.default.weight, model.decoder.layers.9.encoder_attn.q_proj.lora_B.default.weight, model.decoder.layers.10.self_attn.v_proj.lora_A.default.weight, model.decoder.layers.10.self_attn.v_proj.lora_B.default.weight, model.decoder.layers.10.self_attn.q_proj.lora_A.default.weight, model.decoder.layers.10.self_attn.q_proj.lora_B.default.weight, model.decoder.layers.10.encoder_attn.v_proj.lora_A.default.weight, model.decoder.layers.10.encoder_attn.v_proj.lora_B.default.weight, model.decoder.layers.10.encoder_attn.q_proj.lora_A.default.weight, model.decoder.layers.10.encoder_attn.q_proj.lora_B.default.weight, model.decoder.layers.11.self_attn.v_proj.lora_A.default.weight, model.decoder.layers.11.self_attn.v_proj.lora_B.default.weight, model.decoder.layers.11.self_attn.q_proj.lora_A.default.weight, model.decoder.layers.11.self_attn.q_proj.lora_B.default.weight, model.decoder.layers.11.encoder_attn.v_proj.lora_A.default.weight, model.decoder.layers.11.encoder_attn.v_proj.lora_B.default.weight, model.decoder.layers.11.encoder_attn.q_proj.lora_A.default.weight, model.decoder.layers.11.encoder_attn.q_proj.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the merged model\n",
    "merged_model = AutoModelForSeq2SeqLM.from_pretrained(\"../Model/LoRa/M2M100/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'नमस्ते, आप कैसे हैं?'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the translation pipeline\n",
    "translator = pipeline(\"translation_en_to_hi\", model=merged_model, tokenizer=tokenizer)\n",
    "\n",
    "# Test translation\n",
    "text = \"Hello, how are you?\"\n",
    "translated_text = translator(text)\n",
    "\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"\n",
    "    Load the base model and tokenizer.\n",
    "    \"\"\"\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load T5-small model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_lora(model, task_type=\"SEQ_2_SEQ_LM\", r=8, lora_alpha=32, lora_dropout=0.1):\n",
    "    \"\"\"\n",
    "    Configure LoRA for the model.\n",
    "    \"\"\"\n",
    "    lora_config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        target_modules=[\"q\", \"v\"],  # Apply LoRA to attention layers (query and value)\n",
    "        bias=\"none\",\n",
    "        task_type=task_type,\n",
    "    )\n",
    "    return get_peft_model(model, lora_config)\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = configure_lora(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_dataset(data_file, split_percentage=50, train_test_split_ratio=0.2, val_test_split_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Load the dataset and split it into train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"csv\", data_files={\"train\": data_file}, split=f\"train[:{split_percentage}%]\")\n",
    "    \n",
    "    # Split into train and test\n",
    "    train_test_split = dataset.train_test_split(test_size=train_test_split_ratio, seed=42)\n",
    "    \n",
    "    # Further split test into validation and test\n",
    "    val_test_split = train_test_split[\"test\"].train_test_split(test_size=val_test_split_ratio, seed=42)\n",
    "    \n",
    "    # Combine splits into a DatasetDict\n",
    "    raw_dataset = {\n",
    "        \"train\": train_test_split[\"train\"],\n",
    "        \"validation\": val_test_split[\"train\"],\n",
    "        \"test\": val_test_split[\"test\"],\n",
    "    }\n",
    "    \n",
    "    return DatasetDict(raw_dataset)\n",
    "\n",
    "# Load and split the dataset\n",
    "data_file = '../Datasets/processed_data.csv'\n",
    "dataset = load_and_split_dataset(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, source_lang=\"English\", target_lang=\"Hindi\", max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenize and preprocess the dataset for Seq2Seq tasks.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(examples[source_lang], max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "    targets = tokenizer(examples[target_lang], max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, fn_kwargs={\"tokenizer\": tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_collator(tokenizer, model):\n",
    "    \"\"\"\n",
    "    Create a data collator for Seq2Seq tasks.\n",
    "    \"\"\"\n",
    "    return DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Create data collator\n",
    "data_collator = create_data_collator(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_training_args(output_dir, num_train_epochs=3, per_device_train_batch_size=16, fp16=True):\n",
    "    \"\"\"\n",
    "    Configure training arguments for Seq2SeqTrainer.\n",
    "    \"\"\"\n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=5e-4,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        fp16=fp16,\n",
    "    )\n",
    "\n",
    "# Configure training arguments\n",
    "output_dir = \"./lora_t5\"\n",
    "training_args = configure_training_args(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer(model, tokenizer, training_args, train_dataset, eval_dataset, data_collator):\n",
    "    \"\"\"\n",
    "    Create a Seq2SeqTrainer instance.\n",
    "    \"\"\"\n",
    "    return Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "# Create trainer\n",
    "trainer = create_trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    training_args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_merged_model(model, output_dir):\n",
    "    \"\"\"\n",
    "    Merge LoRA layers and save the model.\n",
    "    \"\"\"\n",
    "    model.merge_and_unload()\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "# Save the merged model\n",
    "save_merged_model(model, \"./merged_lora_t5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(trainer, eval_dataset):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test dataset.\n",
    "    \"\"\"\n",
    "    results = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "    print(f\"Evaluation Results: {results}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(trainer, tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load evaluation metric (BLEU)\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "# Postprocess text function\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "# Compute metrics function\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n",
    "# Load model and tokenizer\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Configure LoRA\n",
    "def configure_lora(model, task_type=\"SEQ_2_SEQ_LM\", r=8, lora_alpha=32, lora_dropout=0.1):\n",
    "    lora_config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        target_modules=[\"q\", \"v\"],\n",
    "        bias=\"none\",\n",
    "        task_type=task_type,\n",
    "    )\n",
    "    return get_peft_model(model, lora_config)\n",
    "\n",
    "# Load and split dataset\n",
    "def load_and_split_dataset(data_file, split_percentage=0.1, train_test_split_ratio=0.2, val_test_split_ratio=0.5):\n",
    "    dataset = load_dataset(\"csv\", data_files={\"train\": data_file}, split=f\"train[:{split_percentage}%]\")\n",
    "    train_test_split = dataset.train_test_split(test_size=train_test_split_ratio, seed=42)\n",
    "    val_test_split = train_test_split[\"test\"].train_test_split(test_size=val_test_split_ratio, seed=42)\n",
    "    raw_dataset = {\n",
    "        \"train\": train_test_split[\"train\"],\n",
    "        \"validation\": val_test_split[\"train\"],\n",
    "        \"test\": val_test_split[\"test\"],\n",
    "    }\n",
    "    return DatasetDict(raw_dataset)\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess_function(examples, tokenizer, source_lang=\"English\", target_lang=\"Hindi\", max_length=128):\n",
    "    inputs = tokenizer(examples[source_lang], max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "    targets = tokenizer(examples[target_lang], max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Create data collator\n",
    "def create_data_collator(tokenizer, model):\n",
    "    return DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Configure training arguments\n",
    "def configure_training_args(output_dir, num_train_epochs=3, per_device_train_batch_size=16, fp16=True):\n",
    "    return Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=5e-4,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        fp16=fp16,\n",
    "    )\n",
    "\n",
    "# Create trainer\n",
    "def create_trainer(model, tokenizer, training_args, train_dataset, eval_dataset, data_collator, compute_metrics):\n",
    "    return Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,  # Add compute_metrics to the trainer\n",
    "    )\n",
    "\n",
    "# Save merged model\n",
    "def save_merged_model(model, output_dir):\n",
    "    model.merge_and_unload()\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(trainer, eval_dataset):\n",
    "    results = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "    print(f\"Evaluation Results: {results}\")\n",
    "\n",
    "# Main workflow\n",
    "if __name__ == \"__main__\":\n",
    "    # Load model and tokenizer\n",
    "    model_name = \"t5-small\"\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "\n",
    "    # Apply LoRA\n",
    "    model = configure_lora(model)\n",
    "\n",
    "    # Load and split dataset\n",
    "    data_file = \"../Datasets/processed_data.csv\"\n",
    "    dataset = load_and_split_dataset(data_file)\n",
    "\n",
    "    # Preprocess dataset\n",
    "    tokenized_datasets = dataset.map(preprocess_function, batched=True, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "    # Create data collator\n",
    "    data_collator = create_data_collator(tokenizer, model)\n",
    "\n",
    "    # Configure training arguments\n",
    "    output_dir = \"./lora_t5\"\n",
    "    training_args = configure_training_args(output_dir)\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = create_trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        training_args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,  # Pass compute_metrics to the trainer\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the merged model\n",
    "    save_merged_model(model, \"./merged_lora_t5\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate_model(trainer, tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WPanda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
