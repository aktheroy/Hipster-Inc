{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data loading and spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 40\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load only 20% of the dataset\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"../Datasets/processed_data.csv\"}, split=\"train[:50]\")\n",
    "\n",
    "# Split into train and test sets (e.g., 80% train, 20% test)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Further split the test set into validation and test (e.g., 50-50 split of the 20%)\n",
    "validation_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Combine splits into a DatasetDict\n",
    "raw_dataset = {\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": validation_test_split[\"train\"],\n",
    "    \"test\": validation_test_split[\"test\"]\n",
    "}\n",
    "\n",
    "dataset = DatasetDict(raw_dataset)\n",
    "\n",
    "# Inspect the resulting dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'English': 'death is seen as a boundary to another world.', 'Hindi': 'मौत एक और दुनिया के लिए एक सीमा के रूप में देखा जाता है।'}\n",
      "{'English': '(he later returns to africa, but this part of his life is not recorded in this book.)', 'Hindi': '(बाद में वे अफ्रीका वापस आते हैं, लेकिन उनके जीवन के इस भाग को इस पुस्तक में दर्ज नहीं किया गया है।'}\n",
      "{'English': '10 december 2001), who was a british woman.', 'Hindi': '10 दिसंबर 2001) से हुआ था, जो एक ब्रिटिश महिला थीं।'}\n",
      "{'English': 'can you follow me or shall i follow you?\"', 'Hindi': 'यानि तू मुझ पर हँसा या उस कुम्हार (ईश्वर) पर?'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])\n",
    "print(dataset['train'][1])\n",
    "print(dataset['train'][2])\n",
    "print(dataset['train'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# i.\tFacebook / M2M100 418M\n",
    "- M2M-100 stands for Massively Multilingual Model with 100 languages.\n",
    "- 418M refers to the model size in terms of the number of parameters. Parameters are the learnable weights within the model's neural network. A larger number of parameters generally allows the model to learn more complex patterns and achieve higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
