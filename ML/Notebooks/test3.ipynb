{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset files\n",
    "en_file_path = \"../Datasets/WikiMatrix/WikiMatrix.en-hi.en\"\n",
    "hi_file_path = \"../Datasets/WikiMatrix/WikiMatrix.en-hi.hi\"\n",
    "output_file_path = \"wikimatrix_en_hi.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files and write to CSV\n",
    "with open(en_file_path, \"r\", encoding=\"utf-8\") as en_file, open(hi_file_path, \"r\", encoding=\"utf-8\") as hi_file, open(output_file_path, \"w\", encoding=\"utf-8\", newline=\"\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"English\", \"Hindi\"])  # Write header\n",
    "    for en_sentence, hi_sentence in zip(en_file, hi_file):\n",
    "        writer.writerow([en_sentence.strip(), hi_sentence.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recite in the name of your Lord who created—Created man from a clinging substance.</td>\n",
       "      <td>अपने परवरदिगार का नाम ले कर पढ़ो, जिसने (दुनिया को) पैदा ‎किया।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They were tenants to their lord.</td>\n",
       "      <td>अतः वे अपने इष्ट परम प्रभु की उपासना में ही दत्तचित्त रहते थे।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indeed your Lord is the All-beneficent.</td>\n",
       "      <td>तुम्हारा रब एक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I mean, we all lived in this century.</td>\n",
       "      <td>मेरा मतलब है, हम सभी को इस सदी में रहते थे।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Be steadfastly righteous!</td>\n",
       "      <td>अतः तुम वही करो जो उचित है।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              English  \\\n",
       "0  Recite in the name of your Lord who created—Created man from a clinging substance.   \n",
       "1                                                    They were tenants to their lord.   \n",
       "2                                             Indeed your Lord is the All-beneficent.   \n",
       "3                                               I mean, we all lived in this century.   \n",
       "4                                                           Be steadfastly righteous!   \n",
       "\n",
       "                                                             Hindi  \n",
       "0  अपने परवरदिगार का नाम ले कर पढ़ो, जिसने (दुनिया को) पैदा ‎किया।  \n",
       "1   अतः वे अपने इष्ट परम प्रभु की उपासना में ही दत्तचित्त रहते थे।  \n",
       "2                                               तुम्हारा रब एक है।  \n",
       "3                      मेरा मतलब है, हम सभी को इस सदी में रहते थे।  \n",
       "4                                      अतः तुम वही करो जो उचित है।  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(output_file_path)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 231460 entries, 0 to 231459\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   English  231459 non-null  object\n",
      " 1   Hindi    231459 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce81eae69c0a4fd2b8b7e7af132fdd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['English', 'Hindi'],\n",
       "        num_rows: 231459\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the files and write to CSV\n",
    "with open(en_file_path, \"r\", encoding=\"utf-8\") as en_file, open(hi_file_path, \"r\", encoding=\"utf-8\") as hi_file, open(output_file_path, \"w\", encoding=\"utf-8\", newline=\"\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"English\", \"Hindi\"])  # Write header\n",
    "    \n",
    "    for en_sentence, hi_sentence in zip(en_file, hi_file):\n",
    "        # Strip the sentences and check if they are not empty\n",
    "        en_sentence = en_sentence.strip()\n",
    "        hi_sentence = hi_sentence.strip()\n",
    "        \n",
    "        # Skip writing to the CSV if either sentence is empty or None\n",
    "        if en_sentence and hi_sentence:\n",
    "            writer.writerow([en_sentence, hi_sentence])\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"wikimatrix_en_hi.csv\"})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 40\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Load only 20% of the dataset\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"wikimatrix_en_hi.csv\"}, split=\"train[:50]\")\n",
    "\n",
    "# Split into train and test sets (e.g., 80% train, 20% test)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Further split the test set into validation and test (e.g., 50-50 split of the 20%)\n",
    "validation_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Combine splits into a DatasetDict\n",
    "raw_dataset = {\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": validation_test_split[\"train\"],\n",
    "    \"test\": validation_test_split[\"test\"]\n",
    "}\n",
    "\n",
    "dataset = DatasetDict(raw_dataset)\n",
    "\n",
    "# Inspect the resulting dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'English': 'Death is seen as a boundary to another world.', 'Hindi': 'मौत एक और दुनिया के लिए एक सीमा के रूप में देखा जाता है।'}\n",
      "{'English': '(He later returns to Africa, but this part of his life is not recorded in this book.)', 'Hindi': '(बाद में वे अफ्रीका वापस आते हैं, लेकिन उनके जीवन के इस भाग को इस पुस्तक में दर्ज नहीं किया गया है।'}\n",
      "{'English': '10 December 2001), who was a British woman.', 'Hindi': '10 दिसंबर 2001) से हुआ था, जो एक ब्रिटिश महिला थीं।'}\n",
      "{'English': 'Can you follow me or shall I follow you?\"', 'Hindi': 'यानि तू मुझ पर हँसा या उस कुम्हार (ईश्वर) पर?'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])\n",
    "print(dataset['train'][1])\n",
    "print(dataset['train'][2])\n",
    "print(dataset['train'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import M2M100Config, M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "model_ID = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"en\", tgt_lang=\"hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "source_lang = \"English\"\n",
    "target_lang = \"Hindi\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Extract English and Hindi from the dataset's features\n",
    "    English = examples[source_lang]\n",
    "    Hindi = examples[target_lang]\n",
    "\n",
    "    # Tokenize the input text\n",
    "    model_inputs = tokenizer(English, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Tokenize the target text\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(Hindi, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    # Add tokenized targets as labels\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3636287a49e0415ebe7ddfce8ba38ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akthe\\miniconda3\\envs\\WPanda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3957: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5091399c7000462b9a4fa17fa848db0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19dcc48392174fcb8d16de5e88a90bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['English', 'Hindi', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 40\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['English', 'Hindi', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['English', 'Hindi', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Map the preprocessing function to the DatasetDict\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Inspect the tokenized dataset\n",
    "print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205896f4b3434d26b75dfd0b07f96022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4269f67a7504044a175d51511f34fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2680480480194092, 'eval_bleu': 20.2156, 'eval_gen_len': 23.0, 'eval_runtime': 8.0105, 'eval_samples_per_second': 0.624, 'eval_steps_per_second': 0.125, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akthe\\miniconda3\\envs\\WPanda\\Lib\\site-packages\\transformers\\modeling_utils.py:2748: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52898aad3c34db188afbd9d1a396566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2237743139266968, 'eval_bleu': 33.5161, 'eval_gen_len': 21.8, 'eval_runtime': 7.6274, 'eval_samples_per_second': 0.656, 'eval_steps_per_second': 0.131, 'epoch': 2.0}\n",
      "{'train_runtime': 59.0222, 'train_samples_per_second': 1.355, 'train_steps_per_second': 0.102, 'train_loss': 2.1934680938720703, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=2.1934680938720703, metrics={'train_runtime': 59.0222, 'train_samples_per_second': 1.355, 'train_steps_per_second': 0.102, 'total_flos': 4469653241856.0, 'train_loss': 2.1934680938720703, 'epoch': 2.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"../Modesl\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, #change to bf16=True for XPU\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_ID,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"tfmodel/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'तुम मूर्ख हो'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "text = 'are u dick'\n",
    "translator = pipeline(\"translation_en_to_hi\", model=\"tfmodel/\")\n",
    "translator(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
