{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "# Third-party library imports\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from evaluate import load\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq,\n",
    "    AutoTokenizer, GenerationConfig,\n",
    "    M2M100Config, M2M100ForConditionalGeneration,\n",
    "    M2M100Tokenizer, Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer, pipeline,\n",
    ")\n",
    "\n",
    "# Local application/library specific imports\n",
    "import torch\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data loading and spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 1793\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 224\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 225\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "percent_data_select = \"train[:1%]\" # add percent sign ie. \"train[:20%]\" to select that percent of dama \n",
    "# Load only 20% of the dataset\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"../Datasets/processed_data.csv\"}, split=percent_data_select)\n",
    "\n",
    "# Split into train and test sets (e.g., 80% train, 20% test)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Further split the test set into validation and test (e.g., 50-50 split of the 20%)\n",
    "validation_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Combine splits into a DatasetDict\n",
    "raw_dataset = {\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": validation_test_split[\"train\"],\n",
    "    \"test\": validation_test_split[\"test\"]\n",
    "}\n",
    "\n",
    "dataset = DatasetDict(raw_dataset)\n",
    "\n",
    "# Inspect the resulting dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'English': \"when jackson's group reaches china, their plane runs out of fuel.\", 'Hindi': 'जब जैक्सन का समूह चीन पहुंचता है, तो उनका विमान ईंधन से बाहर निकलता है।'}\n",
      "{'English': 'then he swings to the left and blows the ball loose.', 'Hindi': 'फिर पहले दाईं ओर मुँह फेरता है और तब बाईं ओर।'}\n",
      "{'English': 'it may not always be necessary to treat with four drugs from the beginning.', 'Hindi': 'हमेशा शुरुआत से चार दवाओं के साथ उपचार करना जरुरी नहीं होता है।'}\n",
      "{'English': 'while three machines were built, only one machine was put into operational service.', 'Hindi': 'हालांकि तीन मशीनों का निर्माण किया गया था, उनमें से सिर्फ एक ही मशीन को संचालक सेवा में लिया गया था।'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])\n",
    "print(dataset['train'][1])\n",
    "print(dataset['train'][2])\n",
    "print(dataset['train'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\tGooglE / T5\n",
    "- T5 propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "## Load model directly\n",
    "\n",
    "model_ID = \"google-t5/t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_ID)\n",
    "modelGt5 = AutoModelForSeq2SeqLM.from_pretrained(model_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 TOKENIZING DATASETS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"English\"  # Replace with the actual column name for English in your dataset\n",
    "target_lang = \"Hindi\"    # Replace with the actual column name for Hindi in your dataset\n",
    "prefix = \"translate English to Hindi: \"  # Task prefix for T5\n",
    "\n",
    "# Preprocess function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex for ex in examples[source_lang]]\n",
    "    targets = [ex for ex in examples[target_lang]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['English', 'Hindi', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1793\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['English', 'Hindi', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 224\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['English', 'Hindi', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 225\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'English': 'then he swings to the left and blows the ball loose.', 'Hindi': 'फिर पहले दाईं ओर मुँह फेरता है और तब बाईं ओर।', 'input_ids': [13959, 1566, 12, 25763, 10, 258, 3, 88, 7180, 7, 12, 8, 646, 11, 6019, 7, 8, 1996, 6044, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Hindi Input: translate Hindi to English: हालांकि इससे सात संसदों और सरकारों के लिए अनुमति मिलेगी, पर जब 1980 में समुदाय और क्षेत्र बनाए गए, तो फ्लेमिश राजनीतिज्ञों ने दोनों के विलय का फैसला किया।\n",
      "Input Tokens: ['▁translate', '▁Hindi', '▁to', '▁English', ':', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', ',', '▁', '<unk>', '▁', '<unk>', '▁1980', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', ',', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '▁', '<unk>', '</s>']\n",
      "Decoded Input Text: translate Hindi to English: <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>, <unk> <unk> 1980 <unk> <unk> <unk> <unk> <unk> <unk>, <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk></s>\n",
      "\n",
      "Original English Target: although this would allow for seven parliaments and governments, when the communities and regions were created in 1980, flemish politicians decided to merge both.\n",
      "Target Tokens: ['▁although', '▁this', '▁would', '▁allow', '▁for', '▁seven', '▁parliament', 's', '▁and', '▁governments', ',', '▁when', '▁the', '▁communities', '▁and', '▁regions', '▁were', '▁created', '▁in', '▁1980', ',', '▁fle', 'm', 'ish', '▁politicians', '▁decided', '▁to', '▁merge', '▁both', '.', '</s>']\n",
      "Decoded Target Text: although this would allow for seven parliaments and governments, when the communities and regions were created in 1980, flemish politicians decided to merge both.</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akthe\\miniconda3\\envs\\WPanda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3957: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Sample data from the dataset\n",
    "sample = dataset[\"train\"][12]  \n",
    "input_text = \"translate Hindi to English: \" + sample[\"Hindi\"]\n",
    "target_text = sample[\"English\"]\n",
    "\n",
    "# Tokenize inputs and targets\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(target_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Convert token IDs back to tokens\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "target_tokens = tokenizer.convert_ids_to_tokens(targets[\"input_ids\"][0])\n",
    "\n",
    "# Print the results for inspection\n",
    "print(\"Original Hindi Input:\", input_text)\n",
    "print(\"Input Tokens:\", input_tokens)\n",
    "print(\"Decoded Input Text:\", tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=False))\n",
    "print(\"\\nOriginal English Target:\", target_text)\n",
    "print(\"Target Tokens:\", target_tokens)\n",
    "print(\"Decoded Target Text:\", tokenizer.decode(targets[\"input_ids\"][0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data collator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, particularly for transformer models, a data collator plays a crucial role in preparing batches of data for training. It's essentially a function that takes individual samples and combines them into batches in a way that's efficient and optimized for the model's processing.\n",
    "\n",
    "\n",
    "For sequence-to-sequence tasks like translation, a specialized data collator (often DataCollatorForSeq2Seq) becomes critical. It ensures that:\n",
    "- Source and target sequences are properly aligned\n",
    "- Padding is applied consistently\n",
    "- Attention mechanisms can correctly ignore padded tokens\n",
    "- Labels are prepared in a format that allows for loss calculation during training\n",
    "\n",
    "\n",
    "\n",
    "Without a proper data collator, you'd need to manually handle sequence padding, masking, and batch preparation, which would be computationally expensive and error-prone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=modelGt5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 BLEU (Bilingual Evaluation Understudy): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    \"\"\"\n",
    "    Post-process the predictions and labels by stripping whitespace.\n",
    "    \"\"\"\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]  # SacreBLEU expects a list of references\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    # Check if token IDs are within the valid range\n",
    "    max_token_id = tokenizer.vocab_size - 1\n",
    "    invalid_preds = np.where(preds > max_token_id)\n",
    "    if invalid_preds[0].size > 0:\n",
    "        print(f\"Invalid token IDs found in predictions: {preds[invalid_preds]}\")\n",
    "        preds[invalid_preds] = tokenizer.pad_token_id  # Replace invalid IDs with pad token\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Fine Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akthe\\AppData\\Local\\Temp\\ipykernel_2200\\1094142605.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b946c2163444a28852b949c9e2d435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/675 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7688c4cd1b384301a8c337a9da270c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16295529901981354, 'eval_bleu': 0.0, 'eval_gen_len': 0.0, 'eval_runtime': 15.9433, 'eval_samples_per_second': 14.113, 'eval_steps_per_second': 1.819, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f09f81d7c6a49888f6474237cd6ab33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12182475626468658, 'eval_bleu': 0.0, 'eval_gen_len': 19.9111, 'eval_runtime': 16.0929, 'eval_samples_per_second': 13.981, 'eval_steps_per_second': 1.802, 'epoch': 2.0}\n",
      "{'loss': 0.8367, 'grad_norm': 0.4020611643791199, 'learning_rate': 5.303703703703704e-06, 'epoch': 2.22}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9415401dcc4cf199103d89284ef999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11731509864330292, 'eval_bleu': 0.0, 'eval_gen_len': 20.0, 'eval_runtime': 17.7484, 'eval_samples_per_second': 12.677, 'eval_steps_per_second': 1.634, 'epoch': 3.0}\n",
      "{'train_runtime': 148.0842, 'train_samples_per_second': 36.324, 'train_steps_per_second': 4.558, 'train_loss': 0.6545272346779152, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=675, training_loss=0.6545272346779152, metrics={'train_runtime': 148.0842, 'train_samples_per_second': 36.324, 'train_steps_per_second': 4.558, 'total_flos': 182000887529472.0, 'train_loss': 0.6545272346779152, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"../Model/GT5/Base/Checkpoint_T5/\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, #change to bf16=True for XPU\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=modelGt5,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Saving fine tuned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Model/GT5/Base/GoogleT5/tokenizer_config.json',\n",
       " '../Model/GT5/Base/GoogleT5/special_tokens_map.json',\n",
       " '../Model/GT5/Base/GoogleT5/spiece.model',\n",
       " '../Model/GT5/Base/GoogleT5/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"../Model/GT5/Base/GoogleT5/\")\n",
    "tokenizer.save_pretrained(\"../Model/GT5/Base/GoogleT5/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': '         '}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "text = 'my name is'\n",
    "translator = pipeline(\"translation_en_to_hi\", model=\"../Model/GT5/Base/GoogleT5/\")\n",
    "translator(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.GooglE / T5 + LoRa \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Data loading and spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 3587\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 448\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['English', 'Hindi'],\n",
      "        num_rows: 449\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "percent_data_select = \"train[:2%]\" # add percent sign ie. \"train[:20%]\" to select that percent of dama \n",
    "# Load only 20% of the dataset\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"../Datasets/processed_data.csv\"}, split=percent_data_select)\n",
    "# Split into train and test sets (e.g., 80% train, 20% test)\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Further split the test set into validation and test (e.g., 50-50 split of the 20%)\n",
    "validation_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Combine splits into a DatasetDict\n",
    "raw_dataset = {\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": validation_test_split[\"train\"],\n",
    "    \"test\": validation_test_split[\"test\"]\n",
    "}\n",
    "\n",
    "dataset = DatasetDict(raw_dataset)\n",
    "\n",
    "# Inspect the resulting dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. model loading with lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model_ID = \"google-t5/t5-small\"\n",
    "tokenizer_lora = T5Tokenizer.from_pretrained(model_ID)\n",
    "model_lora = AutoModelForSeq2SeqLM.from_pretrained(model_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Low rank (fewer trainable parameters)\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q\", \"v\"],  # Apply LoRA to attention layers (query and value)\n",
    "    bias=\"none\",  # Specify which biases to train\n",
    "    task_type=\"SEQ_2_SEQ_LM\",  # Task type (sequence-to-sequence)\n",
    ")\n",
    "\n",
    "# Wrap the base model with LoRA\n",
    "model_lora = get_peft_model(model_lora, lora_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Data Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "source_lang = \"English\"\n",
    "target_lang = \"Hindi\"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer_lora(examples[source_lang], max_length=128, padding=\"max_length\", truncation=True)\n",
    "    targets = tokenizer_lora(examples[target_lang], max_length=128, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer_lora, model=model_lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akthe\\AppData\\Local\\Temp\\ipykernel_2200\\31603556.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e563b4bef8e47de9f46f85c15510a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6287f00d988c4419a1259ab2d4f9ee20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid token IDs found in predictions: [32099 32099 32099]\n",
      "{'eval_loss': 0.9357219934463501, 'eval_bleu': 1.1929, 'eval_gen_len': 5.2405, 'eval_runtime': 41.064, 'eval_samples_per_second': 10.934, 'eval_steps_per_second': 1.388, 'epoch': 1.0}\n",
      "{'loss': 6.5256, 'grad_norm': 7.706357479095459, 'learning_rate': 1.2694877505567932e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c30701f91634218a3ca63ca47777bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3692099153995514, 'eval_bleu': 0.0, 'eval_gen_len': 0.1782, 'eval_runtime': 34.9102, 'eval_samples_per_second': 12.862, 'eval_steps_per_second': 1.633, 'epoch': 2.0}\n",
      "{'loss': 0.9059, 'grad_norm': 0.5674149394035339, 'learning_rate': 5.270972531551597e-06, 'epoch': 2.23}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e597af9f65e34712ad78a9362c256416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35184869170188904, 'eval_bleu': 0.0, 'eval_gen_len': 0.1782, 'eval_runtime': 33.3172, 'eval_samples_per_second': 13.477, 'eval_steps_per_second': 1.711, 'epoch': 3.0}\n",
      "{'train_runtime': 286.5367, 'train_samples_per_second': 37.555, 'train_steps_per_second': 4.701, 'train_loss': 2.8929170581616557, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1347, training_loss=2.8929170581616557, metrics={'train_runtime': 286.5367, 'train_samples_per_second': 37.555, 'train_steps_per_second': 4.701, 'total_flos': 366540566298624.0, 'train_loss': 2.8929170581616557, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"../Model/GT5/LoRa/Checkpoint_T5/\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, #change to bf16=True for XPU\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Saving Finetuned Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Model/GT5/LoRa/GoogleT5/tokenizer_config.json',\n",
       " '../Model/GT5/LoRa/GoogleT5/special_tokens_map.json',\n",
       " '../Model/GT5/LoRa/GoogleT5/spiece.model',\n",
       " '../Model/GT5/LoRa/GoogleT5/added_tokens.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lora.merge_and_unload()\n",
    "model_lora.save_pretrained(\"../Model/GT5/LoRa/GoogleT5/\")\n",
    "tokenizer_lora.save_pretrained(\"../Model/GT5/LoRa/GoogleT5/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Testing Finetuned Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Mein Name ist.'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "text = 'my name is'\n",
    "translator = pipeline(\"translation_en_to_hi\", model=\"../Model/GT5/LoRa/GoogleT5/\")\n",
    "translator(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "# Load the datasets\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "def evaluate_model(model, test_dataset):\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=Seq2SeqTrainingArguments(\n",
    "            output_dir=\"../Model/temp\",\n",
    "            per_device_eval_batch_size=8,\n",
    "            predict_with_generate=True,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            logging_dir='./logs',\n",
    "        ),\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    results = trainer.evaluate()\n",
    "    \n",
    "    # Print some predictions for debugging\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    for i in range(min(5, len(predictions.predictions))):\n",
    "        print(f\"Prediction: {tokenizer.decode(predictions.predictions[i], skip_special_tokens=True)}\")\n",
    "        print(f\"Reference: {tokenizer.decode(predictions.label_ids[i], skip_special_tokens=True)}\")\n",
    "    \n",
    "    return results[\"eval_bleu\"], results[\"eval_loss\"]\n",
    "\n",
    "# Load the untrained model\n",
    "untrained_model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "fine_tuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"../Model/GT5/Base/GoogleT5/\")\n",
    "\n",
    "# Load the fine-tuned LoRA model\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\"),\n",
    "    \"../Model/GT5/LoRa/GoogleT5/\"\n",
    ")\n",
    "\n",
    "# Evaluate all models\n",
    "untrained_bleu, untrained_loss = evaluate_model(untrained_model, test_dataset)\n",
    "fine_tuned_bleu, fine_tuned_loss = evaluate_model(fine_tuned_model, test_dataset)\n",
    "lora_bleu, lora_loss = evaluate_model(lora_model, test_dataset)\n",
    "\n",
    "# Plot BLEU scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar([\"Untrained\", \"Fine-Tuned\", \"LoRA\"], [untrained_bleu, fine_tuned_bleu, lora_bleu], color=[\"blue\", \"green\", \"orange\"])\n",
    "plt.ylabel(\"BLEU Score\")\n",
    "plt.title(\"BLEU Score Comparison\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar([\"Untrained\", \"Fine-Tuned\", \"LoRA\"], [untrained_loss, fine_tuned_loss, lora_loss], color=[\"blue\", \"green\", \"orange\"])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Comparison\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WPanda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
